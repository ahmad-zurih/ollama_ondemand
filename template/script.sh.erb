#!/usr/bin/env bash

set -x

# Benchmark info
echo "TIMING - Starting main script at: $(date)"

# Set working directory to home directory
cd "${HOME}"

##############################################################################
# Start the Ollama server in the background
#    Adjust the Singularity path if needed
##############################################################################
apptainer run --nv /storage/research/dsl_shared/solutions/singularity/ollama.sif \
  ollama serve &

# Wait a bit to ensure Ollama server is ready (listening on 127.0.0.1:11411)
sleep 5

##############################################################################
# Load Python / Install dependencies
#    If your site has a valid python module, load it. Otherwise remove or fix.
##############################################################################
<%- unless context.modules.blank? -%>
# Purge the module environment to avoid conflicts
module purge

# Load the require modules
module load <%= context.modules %>

<%- if context.part.include? "gpu" or context.part.include? "gpu-invest" -%>
module load <%= context.cuda_version.blank? ? "CUDA" : context.cuda_version %>
<%- end -%>

# List loaded modules
module list
<%- end -%>

module load python/3.9 &>/dev/null || true  # remove if HPC doesn't have it

python -m pip install --user --upgrade pip
python -m pip install --user streamlit requests

##############################################################################
# Launch Streamlit on the OOD-provided port
#    OOD typically sets $PORT or $WEB_PORT. Fallback to 8080 if not set.
##############################################################################
# Benchmark info
echo "TIMING - Starting streamlit at: $(date)"

# Launch the Streamlit UI server
apptainer run \
    --env BASEURLPATH="$SERVER_BASEURLPATH" \
    --env SERVER_ADDRESS="ondemand.hpc.unibe.ch" \
    --env SERVER_PORT="$SERVER_PORT" \
    --env TOKEN="$TOKEN" \
    $HOME/ondemand/dev/ollama_ondemand/streamlit.sif